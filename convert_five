import warnings
import os
import re
import json
import time
from glob import glob
from tqdm import tqdm
from marker.convert import convert_single_pdf
from marker.output import markdown_exists, save_markdown
from marker.models import load_all_models
from PIL import Image
import logging
import base64
from loguru import logger
import concurrent.futures
import fitz  # PyMuPDF
import hashlib

# 忽略特定的 FutureWarning
warnings.filterwarnings("ignore", category=FutureWarning, message=".*image_processor_class.*")

# 设置环境变量
os.environ["HF_DATASETS_CACHE"] = "./cache_dir"
os.environ["HF_HOME"] = "./cache_dir"
os.environ["HUGGINGFACE_HUB_CACHE"] = "./cache_dir/"
os.environ["TRANSFORMERS_CACHE"] = "./cache_dir/"
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# 定义路径
base_path = '/mnt/workspace/school'
output_dir = '/mnt/workspace/output3'

# 加载所有模型
model_lst = load_all_models()

# 设置日志记录
logging.basicConfig(filename='error_log.log', level=logging.ERROR)

# 定义将输入的 Markdown 文本分解为结构化数据的函数
def structure_markdown(markdown_text):
    structured_data = []
    lines = markdown_text.split('\n')

    for line in lines:
        if line.startswith('# '):
            structured_data.append({'type': 'title', 'content': line[2:]})
        elif line.startswith('## '):
            structured_data.append({'type': 'subtitle', 'content': line[3:]})
        elif re.match(r'!\[.*\]\(.*\)', line):
            structured_data.append({'type': 'image', 'content': line})
        elif line:
            structured_data.append({'type': 'paragraph', 'content': line})

    return structured_data

def calculate_image_hash(image_bytes):
    """计算图像的哈希值"""
    return hashlib.md5(image_bytes).hexdigest()

def extract_images_with_pymupdf(pdf_file_path, output_folder):
    doc = fitz.open(pdf_file_path)
    seen_hashes = set()
    image_mapping = {}
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        images = page.get_images(full=True)
        for img_index, img in enumerate(images):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_bytes = base_image["image"]
            image_ext = base_image["ext"]
            
            # 计算图像的哈希值
            image_hash = calculate_image_hash(image_bytes)
            
            # 检查图像是否已存在
            if image_hash in seen_hashes:
                # print(f"Duplicate image detected on page {page_num+1}, image {img_index+1}. Skipping.")
                continue
            
            seen_hashes.add(image_hash)
            image_name = f"page_{page_num+1}_img_{img_index+1}.{image_ext}"
            image_path = os.path.join(output_folder, image_name)
            with open(image_path, "wb") as img_file:
                img_file.write(image_bytes)
            
            # 记录图像的文件名和哈希值
            image_mapping[f"0_image_{img_index}"] = image_name
    # print("Images extracted successfully!")
    return image_mapping

def convert(pdf_file_path):
    try:
        result = convert_single_pdf(pdf_file_path, model_lst, max_pages=10, langs=['zh', 'en'], batch_multiplier=1, start_page=0)
        
        if not isinstance(result, tuple) or len(result) != 3:
            raise ValueError("convert_single_pdf 返回的结果不是包含三个元素的元组")
        
        full_text, doc_images, out_meta = result
        
        if not isinstance(doc_images, dict):
            raise ValueError("doc_images 不是一个字典")
        
        fname = os.path.basename(pdf_file_path).replace('.pdf', '')
        output_folder = os.path.join(output_dir, fname)
        os.makedirs(output_folder, exist_ok=True)
        
        # 使用 PyMuPDF 提取图像
        image_mapping = extract_images_with_pymupdf(pdf_file_path, output_folder)
        
        # 替换 Markdown 文件中的图像引用
        for key, image_name in image_mapping.items():
            full_text = full_text.replace(f"!({key}.png)", f"!({image_name})")
        
        markdown_file_path = os.path.join(output_folder, fname + '.md')
        
        with open(markdown_file_path, 'w') as f:
            f.write(full_text)
        
        # 保存元数据
        meta_file_path = os.path.join(output_folder, f'{fname}_meta.json')
        with open(meta_file_path, 'w') as meta_f:
            json.dump(out_meta, meta_f, ensure_ascii=False, indent=4)

        # 提取结构化数据
        structured_data = structure_markdown(full_text)

        return {
            'source_pdf': pdf_file_path,
            'markdown_file': markdown_file_path,
            'conversion_time': time.ctime(),
            'structured_data': structured_data,
            'output_folder': output_folder
        }
    except Exception as e:
        logging.error(f"Error processing {pdf_file_path}: {e}")
        return None

def collect_all_target_pdf():
    os.makedirs(base_path, exist_ok=True)
    os.makedirs(output_dir, exist_ok=True)
    
    all_pdf_files = glob(os.path.join(base_path, '*.pdf'))
    print(f"Found {len(all_pdf_files)} PDF files.")
    if not all_pdf_files:
        print("No PDF files found. Please check the base_path and file structure.")

    markdown_files_info = []

    max_workers = 1
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(convert, pdf_file): pdf_file for pdf_file in all_pdf_files}
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(all_pdf_files)):
            try:
                result = future.result()
                if result is not None:
                    markdown_files_info.append(result)
            except Exception as e:
                logging.error(f"Error in future result: {e}")

    with open('result.json', 'w') as f:
        json.dump(markdown_files_info, f, ensure_ascii=False, indent=4)

if __name__ == '__main__':
    collect_all_target_pdf()
